## 操作系统介绍

如果你如果你正在上操作系统课程，你应该已经对计算机运行程序时发生了什么有所了解。如果没有，这本书（以及相应课程）可能对你来说有些困难——因此你应该停止阅读本书，去最近的书店购买相应的背景资料。(此处作者推荐CSAPP)

那么在程序运行时究竟发生了什么呢？

其实一个运行中的程序做的事情很简单：执行指令。处理器以每秒数百万次（甚至数十亿次）的速度从内存取指令，译码（得到具体指令功能），执行指令（就是实现其所指功能，比如将两个数相加、访问内存、检查条件，跳转到某一个函数等等）。一条指令执行完毕后，处理器会紧接着处理下一条，如此反复，直至程序完成（现代处理器为提高程序运行速度做出了很多努力，如一次执行多条指令，甚至是指令可以不按顺序完成，但此处只考虑最简单的情形，即一次执行一条指令，且按顺序执行）。

如上所述，我们描述的是冯诺依曼（Von Neumann）模型的基本状况。听起来很简单不是吗？但在这门课中，我们将会学习在程序执行过程中许多其他的手段，这些手段的目的统一，都是为了让系统更易于使用。

在计算机运行时有一个软件实体帮助我们更容易地执行程序（甚至让你能够在同一时间运行多个程序），还能让程序共享内存，使程序能够和设备交互，以及许许多多有趣的功能。这个软件实体就被称为操作系统，让整个程序系统正确并且有效率的执行是操作系统的职责所在。

操作系统通过一个通用的技术实现这些，我们称之为**虚拟化。**就是说，操作系统将物理资源（处理器，内存或磁盘）转换为一种更加通用有效且易于使用的虚拟形式，因此，我们有时也将操作系统称为**虚拟机**。

> **问题关键：如何将资源虚拟化**
>
> 我们在本书中将要回答的一个中心问题很简单：操作系统如何虚拟化资源？这是关键问题。操作系统为什么要这么做并不是主要问题，因为答案是显然的：为了让系统更易于使用。因此我们将聚焦于如何实现：操作系统采用了哪些机制和策略来实现虚拟化？如何有效率地实现这些？以及需要哪些硬件支持。

最后，由于虚拟化使得许多程序同时运行（共享CPU），并且同时访问其各自的指令和数据（共享内存），同时访问设备（共享磁盘等等），因此操作系统又可以看作是**资源管理器。**CPU，内存，磁盘等都可以看作是系统资源，操作系统充当资源管理者的角色，使得系统运行的更为高效，公平，并完成特定功能。为了更好地理解操作系统的角色，下面来看几个例子。

```c
1 #include <stdio.h>
2 #include <stdlib.h>
3 #include <sys/time.h>
4 #include <assert.h>
5 #include "common.h"
6
7 int
8 main(int argc, char *argv[])
9 {
10     if (argc != 2) {
11         fprintf(stderr, "usage: cpu <string>\n");
12         exit(1);
13     }
14     char *str = argv[1];
15     while (1) {
16     Spin(1);
17     printf("%s\n", str);
18     }
19     return 0;
20 }

Figure 2.1: Simple Example: Code That Loops and Prints (cpu.c)
```

### **2.1 CPU虚拟化**

图2.1是我们的第一个程序，它的功能很简单，主要功能是Spin()函数，即不断地检查时间，满一秒即返回，随后输出用户在命令行传递的字符串，如此重复不断。

我们将此文件存作cpu.c，在单个处理器上编译运行，会得到如下结果：

```
prompt> gcc -o cpu cpu.c -Wall

prompt> ./cpu "A"

A

A

A

A

ˆC

prompt>
```

运行结果不出所料，每秒钟输出一次用户传递的字符串（此处为"A"），值得注意的是，这个程序会永无休止地运行下去，因此我们使用Ctrl—C进行终止。

下面我们对这个程序使用一种不同的运行方式，如下图2.2（在tcsh shell中，使用&关键字可以将当前任务设置为后台任务，并且用分号连接可以实现同时运行多个任务）：

```
prompt> ./cpu A & ; ./cpu B & ; ./cpu C & ; ./cpu D &
[1] 7353
[2] 7354
[3] 7355
[4] 7356
A
B
D
C
A
B
D
C
A
C
B
D
...
Figure 2.2: RunningMany Programs At Once
```

可以看出这时的情况很有意思，尽管我们只有一个处理器，但是好像多个程序在同时运行，到底发生了什么？

是操作系统，它凭借硬件的支持，造就了这样一种”假象“，一种系统具有很多虚拟CPU的假象。将一个或一组有限数量的CPU转化为看上去无限多的CPU，从而“表面上”使很多程序在同时运行，这种技术我们称之为CPU虚拟化，是本书内容的第一部分。

当然还需要一些应用程序接口（APIs）来向操作系统传达你的操作意向，比如运行和终止程序，或者告诉操作系统应该运行哪一个程序。我们在本书中也会讨论这些接口，这些接口也是人机交互的主要方式。

 你或许也已经意识到，同时运行多道程序的能力也带来了一些问题，比如如果两个程序在某一时刻同时要运行，应该运行哪一个呢？这由操作系统的策略（policy）决定，操作系统在很多地方使用了一些策略来解决这类调度问题，当我们学习操作系统底层机制（mechanism）的时候也会学习这些策略（比如多道程序同时运行的能力）。从这个角度都说，操作系统可以看作是资源管理器。

```
1 #include <unistd.h>
2 #include <stdio.h>
3 #include <stdlib.h>
4 #include "common.h"
5
6 int
7 main(int argc, char *argv[])
8 {
9      int *p = malloc(sizeof(int)); // a1
10     assert(p != NULL);
11     printf("(%d) memory address of p: %08x\n",
12     getpid(), (unsigned) p); // a2
13     *p = 0; // a3
14     while (1) {
15         Spin(1);
16         *p = *p + 1;
17     printf("(%d) p: %d\n", getpid(), *p); // a4
18     }
19     return 0;
20 }

Figure 2.3: A Program that Accesses Memory (mem.c)
```

### **2.2内存虚拟化**

现在我们来考虑内存。现代机器的物理内存模型很简单。内存仅仅是一个字节序列。读取内存应该指明数据存储的地址，向内存写入时那个样也要指明将要写入的具体数据和写入地址。

程序运行时，内存时刻都在被访问。一个程序将其所有的数据结构都保存在内存中，并且通过不同的指令去访问，比如加载，存储等等。同时不要忘记指令同样存储在内存中，取指令的时候也是在访问内存。

看一下2.3中的程序，通过malloc()函数分配内存，程序输出结果如下：

```
prompt> ./mem
(2134) memory address of p: 00200000
(2134) p: 1
(2134) p: 2
(2134) p: 3
(2134) p: 4
(2134) p: 5
ˆC
```

这个程序首先分配了一些内存（a1行），然后将分配空间的地址输出（a2行），然后将数字0存储在新分配空间的首部，最后，循环，每间隔1秒将地址p中存储的值加1，在每个输出的语句中，还输出了正在执行的程序的进程号（PID），对于每个正在运行的程序，PID唯一。

当然，第一个运行的结果完全在意料之中，下面来看看如果同时运行这个程序的多个实例会发生什么：

```
prompt> ./mem &; ./mem &
[1] 24113
[2] 24114
(24113) memory address of p: 00200000
(24114) memory address of p: 00200000
(24113) p: 1
(24114) p: 1
(24114) p: 2
(24113) p: 2
(24113) p: 3
(24114) p: 3
(24113) p: 4
(24114) p: 4
...
Figure 2.4: Running TheMemory ProgramMultiple Times
```

如图2.4所示，两个进程在相同的地址00200000分配了空间，但是却各自独立地更新存储在该地址的值，这看上去好像是每个进程都有独立的地址空间。

其实这就是操作系统实现的内存虚拟化，每个进程访问其私有的虚拟地址空间（或简称地址空间），操作系统负责将虚拟地址映射到具体的物理地址。一个进程访问其地址空间中的地址时并不影响其他进程（也包括操作系统本身），从每个进程的角度，好像其本身独占所有内存空间，然而事实是，物理内存是被所有程序共享的，由操作系统管理，如何管理内存也是本书第一部分讨论的内容：虚拟化。

### **2.3并发**

本书的另一个重要的主题是**并发**，我们使用这个概念来描述一个程序在同时执行多项任务时所发生的必须被处理的问题。操作系统本身也会产生并发的问题，正如同我们在上述虚拟化的例子中看到的，操作系统需要同时应付许多事情，首先执行一个进程，紧接着下一个，如此类推，因此这种情形下会产生许多有趣的问题。

```
1 #include <stdio.h>
2 #include <stdlib.h>
3 #include "common.h"
4
5 volatile int counter = 0;
6 int loops;
7
8 void *worker(void *arg) {
9     int i;
10     for (i = 0; i < loops; i++) {
11         counter++;
12     }
13     return NULL;
14 }
15
16 int
17 main(int argc, char *argv[])
18 {
19     if (argc != 2) {
20     fprintf(stderr, "usage: threads <value>\n");
21     exit(1);
22     }
23     loops = atoi(argv[1]);
24     pthread_t p1, p2;
25     printf("Initial value : %d\n", counter);
26
27     Pthread_create(&p1, NULL, worker, NULL);
28     Pthread_create(&p2, NULL, worker, NULL);
29     Pthread_join(p1, NULL);
30     Pthread_join(p2, NULL);
31     printf("Final value : %d\n", counter);
32     return 0;
33 }

Figure 2.5: AMulti-threaded Program (threads.c)
```

并发性的问题也不仅仅存在于操作系统本身中，实际上，现在流行的**多线程程序**也存在同样的问题，如图2.5就是一个多线程的例子。

或许你现在可能还不能完全理解这个例子，在本书后续部分会详细讨论。这个程序通用Pthread.creat()（实际调用的是小写形式pthread.creat()，首字母大写的函数是一种封装后的形式，比如封装了异常处理等，在本书后面有许多类似的用法，这应该也是一种优良的编码风格，值得学习）。你可以将线程看作是和其他函数运行在同一内存空间内的一种函数，在同一时刻有多个线程处于活动状态，在这个例子中，每个线程都运行了一个worker()函数，该函数的功能是循环并且使用counter变量记录循环的次数。

下面是我们将loops变量设置为1000时的运行结果，loops变量决定了两个线程中各自执行循环并增加counter变量的次数，那么当loops为1000时，你期望最终counter的值是多少呢？

```
prompt> gcc -o thread thread.c -Wall -pthread
prompt> ./thread 1000
Initial value : 0
Final value : 2000
```

你可能已经猜到，最终值是2000，因为两个线程各自都执行了1000次counter++操作。实际上，当我们将loops设置为N时，我们会默认最终的counter值应该是2N，然而事实果真如此？其实事情并不简单，让我们看看当把loops值设置大些时会发生什么：

```
prompt> ./thread 100000
Initial value : 0
Final value : 143012 // huh?? 蛤？
prompt> ./thread 100000
Initial value : 0
Final value : 137298 // what the?? 什么鬼？？
```

当loops值设置为100000时，得到的结果不是期望中的200000，而是143012，更奇怪的是当我们重新执行一遍时居然又得到了另外一个结果，当你不断执行这个程序，你会得到许多不同的结果，或许偶尔会有你期望的答案200000，但是，为何如此？究竟发生了什么？

实际上，这种奇怪的输出结果和指令的执行过程有关，如前假设，计算机一次执行一条指令

，而counter++语句虽然只是一条语句，但却包含三条指令：一条将counter的值从内存加载到寄存器；一条将寄存器中的counter值增加1；最后一条将其从寄存器转存到内存中。由于这三条指令并不是原子化地执行（所谓原子化是指所有指令紧接着执行，中间不被其他指令打断），从而发生了这些奇怪的现象，我们将在本书的第二部分详细讨论。

> **问题的关键：如果构建正确的并发程序？**
>
> 当多个线程在同一内存空间内并发执行时，如何构建正确的程序？操作系统需要提供哪些原语支持？（原语，primitives，是由若干条指令组成的,用于完成一定功能的一个过2程）硬件方面需要提供哪些机制？我们如何使用这些来解决并发问题？

### **2.4 持久化**

第三个重要的主题就是**持久。**由于在动态随机存储器（DRAM）中数据具有不稳定性，因此在系统内存中，数据具有易失性；因此当断电或系统崩溃时，内存中的数据会丢失。从而我们需要软件和硬件配合来存储数据，并使数据具有持久性，数据至上，因此这种存储对任何系统来说都很重要。

其中硬件以I/O设备的形式展现；在现代系统中，（磁头式）硬盘是常用的长时间存储设备，尽管目前固态硬盘也发展的很快。

操作系统中管理磁盘的软件称作**文件系统；**它负责将用户创建的文件可靠且高效地存储到磁盘上。

不像CPU和内存那样虚拟化的机制，操作系统并不为每一个应用创建私有且虚拟的磁盘。它假设用户经常需要在文件之间共享信息，比如当写一个C程序时，需要一个编辑器来创建和编辑源文件，然后需要编译器将源代码变成可执行文件。当一切完成可以使用命令来运行该程序。可以看出文件在各个过程中共享，编辑器创建的文件作为编译器的输入，编译器产生的可执行文件提供给系统运行。

为了更好地理解，下面来看一段代码，如图2.6，该代码实现的功能是创建了一个文件(/temp/file)，文件中包含了字符串"hello world"：

```
1 #include <stdio.h>
2 #include <unistd.h>
3 #include <assert.h>
4 #include <fcntl.h>
5 #include <sys/types.h>
6
7 int
8 main(int argc, char *argv[])
9 {
10     int fd = open("/tmp/file", O_WRONLY | O_CREAT | O_TRUNC, S_IRWXU);
11     assert(fd > -1);
12     int rc = write(fd, "hello world\n", 13);
13     assert(rc == 13);
14     close(fd);
15     return 0;
16 }
Figure 2.6: A Program That Does I/O (io.c)
```

为了完成这一任务，该程序进行了三次系统调用，首先使用open()，打开并创建文件，然后使用write()向文件中写入数据，最后使用close()关闭文件表明系统不会再写入任何数据。操作中和这些系统调用关联的部分称为文件系统，文件系统负责处理这些请求并返回结果或错误代码给用户。

你可能会疑惑操作系统在向磁盘中写入时究竟做了什么，这其实有些复杂：首先文件系统会算出数据在磁盘上将要存放的位置，然后用文件系统中不同的数据结构记录下来。这些需要对底层存储设备发起I/O请求。任何写过设备驱动的人都知道，让具体的硬件设备完成特定的任务是一项负责且充满细节的工作，它需要对底层硬件设备和其运行机理有着深刻的理解。所幸的是，操作系统通过系统调用为我们操纵硬件设备提供了简明标准的方式，从这个层面，操作系统可以看作是一种标准库。

当然，对于硬件设备如何访问以及文件系统如何保持数据持久化方面有着许许多多的细节。从性能方面说，大多数文件系统对于写操作采取延迟处理策略，以期通过批处理的方法提高效率。为了处理写入时系统崩溃的问题，许多文件系统采取了一系列复杂的写入协议，比如日志（journaling）或写时复制（copy—on-write），通过谨慎地处理写入的顺序从而在写入过程中发生系统崩溃时能够正确的恢复而保证文件的完整性和一致性。为了提高操作效率，文件系统还采用了许多数据结构和访问方法，从简单的链表到复杂的B-树均有所涉及。我们将在本书的第三部分详细讨论这些，包括设备、I/O、以及磁盘、RAID，最后到文件系统。

> **问题的关键：如何持久化存储数据**
>
> 文件系统是操作系统中负责管理持久化数据的部分。做到这些需要哪些技术呢？要想达到高性能需要哪些机制和策略？在软件和硬件都有可能崩溃的情况下，如何保证数据的可靠性？

### **2.5 设计目标**

现在你应该已经理解了一些操作系统究竟做了些什么：将物理资源，如CPU、内存、磁盘等虚拟化；处理和并发有关等复杂而有挑战的问题；还要持久性地保存文件，保证其安全长效。如果我们想构建这样一个系统，我们需要设立一些目标来帮助我们集中设计和实现，并且在必要时做出适当的**权衡**；构建一个系统最终要就是在各项目标之间权衡。

首先最基本的目标就是设立一些**抽象**来使得系统更加易于使用。抽象是计算机科学中的核心概念，抽象使得构建大型程序成为可能。通过抽象，我们将大型程序分解为小且易于理解的程序片；通过抽象，我们可以使用高级语言编写程序而不用考虑汇编细节；通过抽象，我们可以使用汇编语言而不用考虑逻辑门；通过抽象，我们可以通过逻辑门设计处理器而不用考虑晶体管的原理，如此等等，抽象为我们提供了一种将复杂问题简单化层次化的方法，在计算机科学中极为重要，在操作系统的学习中更要深刻体会这个概念。

另一个目标就是要使得操作系统具有**高性能**，换句话说，要使得操作**系统的开销**最小化。虚拟化是一项方便且值得去实现的技术，但并不是不惜一切地去实现，因此我们必须努力使系统在实现虚拟化的同时不产生过大的开销，开销体现在各种各样的方面：额外的时间（过多的指令），额外的空间（内存或磁盘）等。我们将采取各种方式以降低时间空间复杂性。我们还必须明白，完美的解决方案未必存在，我们要学会让步和折中。

还有一个目标是提供应用程序之间以及操作系统和应用程序之间的**保护机制**。由于我们希望多道程序同时运行,因此我们需要一个程序的崩溃和错误不影响其他程序，我们更不希望某个程序能够损坏操作系统。因此，保护机制是设计一个操作系统的主要原则，即隔离机制：将进程之间相互隔离是操作系统必须实现的功能。

操作系统需要持续不休地运行，当其崩溃时，运行在其之上的应用也会随之崩溃，因此操作系统需要具有高度可靠性。随着操作系统的复杂性日益增长（有时甚至超过数百万行代码），构建一个可靠的系统确实是一项挑战，也是目前研究的热点。

其他一些目标也同样很有意义：比如节能环保，能耗高效；互联网时代的安全性；以及移动性对微型设备的重要性。根据使用场景的不同，操作系统的设计目标也不尽相同，但有一些设计目标和原则是具有跨平台性和普适性的。

### **2.6一些历史**

下面简要介绍一下操作系统发展历程中一些重要的进展：

**早期操作系统：仅仅是一种库**

最初，操作系统并没有做太多可以事，仅仅可以看作是常用功能函数的库，比如提供I/O功能的API，把开发者从底层代码中解放出来。

在这些古老的系统上通常一次只能运行一个程序，很多现代操作系统自动完成的事，比如任务调度等，当时都是由人工完成。你甚至可能因为和操作员关系好而让你的程序被优先执行。

这种模式被称为批处理，指一系列任务被设定成一整个批次，然后由操作员按批执行的模式。计算机在这种模式下并不是一种交互式的使用，很浪费人力物力。

**超越库：保护**

在库时代发展之后，操作系统在管理机器方面发挥了更重要的作用。一个重要的方面就是人们意识到操作系统本身的代码可以控制硬件设备，因此应该和普通应用代码区别对待。为什么呢，想象一下如果允许任何应用去读取磁盘的任何地方，那就毫无隐私可言，从而文件系统的实现也就没有那么大的意义了，因此，还需要一些其他的内容。

进而，Atlas系统中首先提出了系统调用这一概念。通过增加一些硬件指令和硬件状态，从而使进入操作系统这一转变过程更为规范和可控，而不是仅仅提供操作系统库函数，通过过程调用来使用。

系统调用和过程调用的核心区别在于系统调用将控制权转交给操作系统同时提升硬件特权级（内核态）。用户应用运行于用户态，意味着应用的行为受到硬件限制；比如用户态的应用不能发起对磁盘的I/O请求，也不能访问内存页，不能向网络上发送数据包。当系统调用被发起时（通常通过一个特殊的硬件指令：陷入）,硬件将控制权转交给陷阱处理器（操作系统预设），同时特权级转变为内核态。在内核态，操作系统可以自由访问控制硬件，比如可以发起I/O或为程序分配更多内存。当操作系统完成请求的服务够会通过return-from-trap指令回到用户态，同时将控制权转交给用户程序。

**多道程序设计的时代**

操作系统真正取得成功是在计算技术本身超越主机设备的时代，即迷你计算机的出现。从大型机到迷你设备，成本的降低让更多的人可以享受计算技术，从而也鼓励的开发者，聪明的人不断完善操作系统，让操作系统能够完成更多有趣美丽的事。

多道程序设计的出现是出于更好利用机器资源的愿景。操作系统将许多任务加载到内存中，并且快速切换，从而提高CPU利用效率。这种切换很重要，因为I/O设备较为缓慢，当一个任务在进行I/O传输时要是还占用CPU会是一种浪费，因为为何不切换到另一个任务运行一段时间？

对多道程序以及在I/O或中断时切换任务的需求促使了操作系统很多概念上的创新发展。比如内存保护：我们不希望一个程序能够访问到另一个程序的内存空间；还有对并发的处理同样具有挑战，在接下来将会具体学习。

一个重要的发展是UNIX系统的出现，主要归功于贝尔实验室的Ken Thompson、Dennis Ritchie。UNIX集成了很多系统的优点，主要来自Multics、TENEX以及Berkeley Time-Sharing System,但是又比他们更易于使用，随后很多人都加入到UNIX系统的完善中来。

**当代操作系统**

当代的主要计算机是个人计算机PC，由Apple和IBM主导，个人计算机成为当今计算技术的主力军。

但不幸的是对于操作系统来说，PC机并未汲取之前的设计经验。比如对于DOS系统（Disk Operating System），就并未考虑内存保护的重要性，一些应用可能会去滥用内存；Mac OS的第一代系统采用非抢占式任务调度，当一个线程陷入死循环同样可以接管整个系统，从而导致系统无响应，必须重启。此时的操作系统缺失的大量特性。

幸运地是，桌面系统随后逐渐得到完善，上一代微型机系统在桌面设备上也逐渐找到了自己的位置。比如Mac OS X 以UNIX为内核，包括了所有应有的成熟特性。Windows也博采众长，从NT开始稳步发展，成为了成熟的系统。今天的手机也在运行着基于Linux的系统。操作系统的设计理念和特性一直在传承和发展。

> **UNIX的重要性**
>
> 在操作系统发展的历史进程中，UNIX的重要性不言而喻。受早期系统影响（尤其是MIT Multics），UNIX汲取并创新了一系列强有力的功能。
>
> 最初贝尔实验室设计UNIX是基于这样的理念：将小有力的程序连接起来从而形成更大的工作群。当在shell内键入命令时，一些机制比如管道等可以很容易将程序串起来形成更大的任务，比如为了在一个文本文件内寻找包含“foo”的行，并且输出总共的行数，可以使用grep foo file.txt|wc -l命令，从而使用grep和wc两个命令实现。
>
> UNIX提供了C语言编译环境，对开发人员十分友好，开发人员可以开发自己的应用并且分享，这种分享同时也促进的UNIX的传播与发展，从而形成了一种开源的理念。
>
> 另一个重要性在于UNIX的源代码是公开且易读的。C语言写的短小优美的内核可以吸引很多开发者去添加新功能。比如Berkeley的Bill Joy开发的BSD系统。后续Joy成立了Sun Microsystems公司。
>
> 不幸的是，由于一些公司想将UNIX据为己有并欲以之获利，因此阻碍了UNIX的继续发展与传播。许多公司都拥有其独自的衍生版本，由此官司不断，争端不断，但是却忽略了系统本身的发展，因此逐渐失去了市场，尤其是Windows出现之后。

> **横空出世的Linux**
>
> 一个芬兰小伙Linus Torvalds基于UNIX的设计理念，自己独立开发了一套内核，并且将其开发的版本开源，由全世界的技术人员进行完善，这对UNIX来说，无疑是一件幸事。
>
> 由于开源、免费和没有法律纠纷，许多公司选择了使用Linux，包括移动端的操作系统，Android,以及苹果公司的操作系统，很多都是基于Linux，很难想象没有Linux，这些公司将会何去何从。每个人都应感谢Linux。

**2.7总结**

以上就是对操作系统的简单介绍。今天的操作系统非常易于使用，其中的主要技术和手段我们会在全书中逐渐介绍。

遗憾的是，由于时间有限，我们并不能介绍操作系统的全部，比如网络、图形化设备、以及系统安全，我们都不会详细介绍，会有专门的课程去研究探讨。

我们关注的重点在于最基本的原理，比如CPU和内存的虚拟化、并发以及文件系统和设备的持久性。了解这些基本原理将会为你打开操作系统世界的大门。

""